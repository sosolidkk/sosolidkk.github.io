<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-11-24T00:32:35-03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">João Pedro</title><subtitle>Hey there! Just created this place to share some thoughts and let my mind loose itself a little bit.  Don't take the things here too seriously. Enjoy your stay!</subtitle><entry><title type="html">Scraping vs Crawling</title><link href="http://localhost:4000/scraping,/crawling/2020/05/18/scraping-vs-crawling.html" rel="alternate" type="text/html" title="Scraping vs Crawling" /><published>2020-05-18T22:02:37-03:00</published><updated>2020-05-18T22:02:37-03:00</updated><id>http://localhost:4000/scraping,/crawling/2020/05/18/scraping-vs-crawling</id><content type="html" xml:base="http://localhost:4000/scraping,/crawling/2020/05/18/scraping-vs-crawling.html">&lt;p&gt;Geralmente esses termos são difundidos de maneira equivocada. Para entender melhor, é necessário saber o que cada um deles significa. Scraping, ou aportuguesando um pouco podemos chegar em “garimpagem”, que é o ato de extrair informações e posteriormente salvá-las em algum local (geralmente uma base de dados) para depois serem analizadas. Basicamente, scraping não se limita somente a web já que é possível realizar scraping de um site, de um banco de dados, de arquivos, as opções são diversas. Já um crawler, popurlamente conhecido como bot ou spider (World wide web, spider, web, pegou o trocadilho?), vai buscar informações ou salvar páginas web completas para depois analizar as informações. Conseguiu perceber a diferença entre eles? Um crawler é exclusivamente para web e se difere tanto em escala quanto alcance de um simples script que realiza scraping.&lt;/p&gt;

&lt;p&gt;Para melhorar o entendimento, eu vou explicar o processo de funcionamento de um crawler.&lt;/p&gt;

&lt;p&gt;Um web crawler inicia com uma lista de URLs para visitar, conhecidas como seeds. A medida que o crawler visita essas URLs, ele primeiro vai buscar novos links para adicionar em uma outra lista chamada de crawl frontier ou fronteira de raspagem. Essa nova lista é uma estrutura de dados para salvar URLs que são elegíveis para raspagem. Caso não tenha ideia, compare a lista à uma fila de prioridade.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Essa fronteira é um dos componentes que torna possível a arquitetura de um crawler. Ela que contém a lógica e as políticas que as spiders seguem quando vão visitas os sites. Nas políticas é possível incluir coisas como qual página deve ser visitada, prioridades que devem ser buscadas em cada página e em qual frequência uma página deve ser visitada.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Após esse processo de salvamento das novas URLs, as spiders vão começar a arquivas as páginas (tipo o “Salvar como” de um navegador) em snapshots de modo que seja possível visualizar, ler e navegar como se fosse o próprio site.&lt;/p&gt;

&lt;p&gt;É necessário ter alguns cuidados quando se usa um web crawler:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A internet é enorme. Todos os dias é criado conteúdo novo, muitas vezes sobre um mesmo assunto já existente que já foi postado e difundido em diversos blogs (como essa mesma explicação que eu estou fazendo). Com isso, é necessário se ter um cuidado extra na hora de programar um crawler para evitar um possível problema de duplicação de dados, seja no ato da captura ou no momento em que os dados serão salvos no banco de dados;&lt;/li&gt;
  &lt;li&gt;Aproveitando o gancho do ponto um, o número de sites que geram URLs pelo lado servidor (server-side) torna difícil o processo dos crawlers na tentativa de evitar conteúdo duplicado em suas buscas. Existem infintas combinações de parâmetros nas URLs, no qual apenas uma porção dessas vai de fato retornar um conteúdo único (que ainda não foi capturado ou que não se encontra no banco de daos). Por exemplo, uma galeria de fotos pode oferecer algumas opções para um usuário como:
    &lt;ul&gt;
      &lt;li&gt;Três modos de ordernas uma imagem;&lt;/li&gt;
      &lt;li&gt;Quatro escolhas de tamanho de thumbnail;&lt;/li&gt;
      &lt;li&gt;Cinco formatos de arquivos;&lt;/li&gt;
      &lt;li&gt;Com isso eu tenho um número enorme de URLs para acessar o mesmo conteúdo. Essa infinitude de combinações é um dos problemas que as spiders tem que enfrentar, visto que eles precisam ordernar entre várias URLs para trazer algo novo e único.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Outro problema é a coordenação das spiders para requisições sucessivas. Elas precisam ser “educadas” o suficiente para saber que existe um limite de requisição em todo servidor que eles permitem e eventualmente evitar um estresse desnecessário. Vale lembrar que todo site tem sua política e regras, no qual nem todos permitem fazer crawling de seu conteudo. Tenham isso em mente pois é muito importante;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Por fim, mas não menos importante, é necessário definir crawling agents, onde cada um irá gerenciar um nincho diferente de spiders e seus propósitos. Existem sites sobre os mais diversos assuntos e é indicado criar políticas de crawl para um desses sites.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Com isso, finalizo esse post e espero que tenha ficado um pouco mais clara a diferença entre esses dois termos.&lt;/p&gt;</content><author><name></name></author><category term="scraping," /><category term="crawling" /><summary type="html">Geralmente esses termos são difundidos de maneira equivocada. Para entender melhor, é necessário saber o que cada um deles significa. Scraping, ou aportuguesando um pouco podemos chegar em “garimpagem”, que é o ato de extrair informações e posteriormente salvá-las em algum local (geralmente uma base de dados) para depois serem analizadas. Basicamente, scraping não se limita somente a web já que é possível realizar scraping de um site, de um banco de dados, de arquivos, as opções são diversas. Já um crawler, popurlamente conhecido como bot ou spider (World wide web, spider, web, pegou o trocadilho?), vai buscar informações ou salvar páginas web completas para depois analizar as informações. Conseguiu perceber a diferença entre eles? Um crawler é exclusivamente para web e se difere tanto em escala quanto alcance de um simples script que realiza scraping.</summary></entry></feed>